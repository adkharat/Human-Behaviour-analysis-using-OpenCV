{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"C:\\\\Users\\\\Ajay\\\\Employee behaviour\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Image in a Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a cascade Classifier \n",
    "#It has feature XML file as input i.e. XML file containt face feature\n",
    "face_cascade = cv2.CascadeClassifier(input_path+\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "#Reading the image as:\n",
    "\n",
    "#Colored image parameter as 0\n",
    "img = cv2.imread(input_path + \"sample4.jpg\",1)\n",
    "\n",
    "#Gray scaled image parameter as 0\n",
    "#img = cv2.imread(input_path + \"sample1.jpg\",0)\n",
    "\n",
    "#Reading image as gray scaled image\n",
    "gray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Using detectMultiScale method to Search the rectangular coordinate of FACE in image; to figure out where is the face\n",
    "faces = face_cascade.detectMultiScale(gray_img,  scaleFactor = 1.05, minNeighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img) #3D matrix for colored image and 2D for gray scaled\n",
    "print(type(img)) #image will be stored as numpy array\n",
    "print(img.shape) #1174 ROW, 1300 COLUMN, 3 Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to add rectangular FACE BOX to face use rectangle() mehtod\n",
    "for x,y,w,h in faces:\n",
    "    img = cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 3)\n",
    "\n",
    "\n",
    "#to display the image in window use imshow()\n",
    "cv2.imshow(\"ajay\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture first Image from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video is multiple frames displayed very quickly\n",
    "video = cv2.VideoCapture(0) #0 is for internal webcam and 1 is for external\n",
    "\n",
    "#Show the camera in a Window\n",
    "check, frame = video.read() \n",
    "print(check)#check is bool type it return whether python is able to read the VideoCapture object\n",
    "print(frame)#frame is a numy array (image), it represent first image that video captures\n",
    "\n",
    "time.sleep(10) #Holf the camera for 10 Secs\n",
    "cv2.imshow('Capturing Camera',frame)\n",
    "cv2.waitKey(0)\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture entire video with help of loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture video we will use while loop. Until, 'CHECK' is true , python will display the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video is multiple frames displayed very quickly\n",
    "video = cv2.VideoCapture(0) #0 is for internal webcam and 1 is for external\n",
    "\n",
    "a=1\n",
    "\n",
    "while True:\n",
    "    a= a+1\n",
    "    check,frame = video.read()\n",
    "    print(frame) #It is RGB image\n",
    "    \n",
    "    gray_img = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) # It is GRAY Image\n",
    "    \n",
    "    cv2.imshow('Capturing Camera',gray_img)\n",
    "                          #If we specify 0 , the movement we press a key the window closes\n",
    "    key = cv2.waitKey(1) #We are generating the new frame after every 1ms sec\n",
    "                            #After every 1ms frame Window will distroy\n",
    "    \n",
    "    if key == ord('q'):     #If user press Q from keyboard then terminate the loop\n",
    "        break\n",
    "    \n",
    "print(a) #print no of frame we have captured\n",
    "video.release() #Free the camera resource\n",
    "cv2.destroyAllWindows() #Kill all window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organisation that study human behaviour. Our task is to give them how long a person was infront of a camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STEP1 : Save the initial image in a FRAME (Image w/o Object)\n",
    "#STEP2 : Convert this image in a Gaussian blur image\n",
    "#STEP3 : Take a next frame that appear after 1st frame (Image w/ object) and also convet it in a gaussian blur image\n",
    "#STEP4 : Calculate the difference. This difference will tell us whether there is a object infront of camera or not. If there is change from 1st frame to 2nd frame then there is definitely a object. But that object might be too small and I don't want these object.\n",
    "#STEP5 : Define the threshold to remove the showdow(mosquito) and other noise\n",
    "#STEP6 : Define the border object\n",
    "#STEP7 : Add the rectangular box around the object\n",
    "#STEP8 : Finally, we calculate the time when object appear and exit the frame. We save that in a dataframe and after that we will visualize that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time, pandas\n",
    "from datetime import datetime\n",
    "\n",
    "first_frame = None\n",
    "status_list = [None,None]\n",
    "time = []\n",
    "df = pandas.DataFrame(columns=[\"start\",\"End\"])\n",
    "\n",
    "video = cv2.VideoCapture(0) #Conver the video capture object to record video using webcam\n",
    "\n",
    "while True:\n",
    "    check , frame = video.read()\n",
    "    status = 0\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert the frame color to gray scale\n",
    "    gray = cv2.GaussianBlur(gray, (21,21),0) #Convert the gray scale frme to GaussianBlur image\n",
    "\n",
    "    #This is used to store the first image of the video\n",
    "    if first_frame is None:\n",
    "        first_frame = gray\n",
    "\n",
    "    #Delta frame\n",
    "    #Compare first frame to current frame\n",
    "    delta_frame = cv2.absdiff(first_frame, gray)\n",
    "    thresh_delta = cv2.threshold(delta_frame, 30, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh_delta = cv2.dilate(thresh_delta, None, iterations=0)\n",
    "    \n",
    "    #Contours\n",
    "    cnts, hierarchy = cv2.findContours(thresh_delta.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)#Draw rectangle for moving objects\n",
    "    #(_, cnts, _) = cv2.findContours(thresh_delta.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #define the contour arear. Basically , add the border\n",
    "\n",
    "    #Draw rectangle for moving objects\n",
    "    for contour in cnts:\n",
    "        if cv2.contourArea(contour) < 1000:\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "\n",
    "    #Show the captured frame from video in a window\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('Capturing',gray)\n",
    "    cv2.imshow('delta',delta_frame)\n",
    "    cv2.imshow('thresh',thresh_delta)\n",
    "    \n",
    "    #Set keyborad key q=quit\n",
    "    key = cv2.waitKey(1)\n",
    "    if(key == ord('q')):\n",
    "        break\n",
    "        \n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Motion timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[datetime.datetime(2020, 6, 12, 11, 41, 21, 677300), datetime.datetime(2020, 6, 12, 11, 41, 21, 923146), datetime.datetime(2020, 6, 12, 11, 41, 22, 63135), datetime.datetime(2020, 6, 12, 11, 41, 34, 939890)]\n"
     ]
    }
   ],
   "source": [
    "import cv2, time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "first_frame = None\n",
    "status_list = [None,None]\n",
    "times = []\n",
    "df = pd.DataFrame(columns=[\"Start\",\"End\"])\n",
    "\n",
    "video = cv2.VideoCapture(0) #Conver the video capture object to record video using webcam\n",
    "\n",
    "while True:\n",
    "    check , frame = video.read()\n",
    "    status = 0\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert the frame color to gray scale\n",
    "    gray = cv2.GaussianBlur(gray, (21,21),0) #Convert the gray scale frme to GaussianBlur image\n",
    "\n",
    "    #This is used to store the first image of the video\n",
    "    if first_frame is None:\n",
    "        first_frame = gray\n",
    "\n",
    "    #Delta frame\n",
    "    #Compare first frame to current frame\n",
    "    delta_frame = cv2.absdiff(first_frame, gray)\n",
    "    thresh_delta = cv2.threshold(delta_frame, 30, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh_delta = cv2.dilate(thresh_delta, None, iterations=0)\n",
    "    \n",
    "    #Contours\n",
    "    cnts, hierarchy = cv2.findContours(thresh_delta.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)#Draw rectangle for moving objects\n",
    "    #(_, cnts, _) = cv2.findContours(thresh_delta.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #define the contour arear. Basically , add the border\n",
    "\n",
    "    #Draw rectangle for moving objects\n",
    "    for contour in cnts:\n",
    "        if cv2.contourArea(contour) < 1000:\n",
    "            continue\n",
    "        status = 1\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "        \n",
    "        \n",
    "    # Motion time\n",
    "    status_list.append(status) #Capture staring Time\n",
    "\n",
    "    if status_list[-1]==1 and status_list[-2]==0: #It is a 1st frame\n",
    "        times.append(datetime.now())\n",
    "    if status_list[-1]==0 and status_list[-2]==1: #Its is a last frame\n",
    "        times.append(datetime.now())\n",
    "\n",
    "    #Show the captured frame from video in a window\n",
    "    cv2.imshow(\"First Frame\", first_frame)\n",
    "    cv2.imshow('Caturing frames', frame)\n",
    "    cv2.imshow('Capturing',gray)\n",
    "    cv2.imshow('delta',delta_frame)\n",
    "    cv2.imshow('thresh',thresh_delta)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Set q=quit\n",
    "    key = cv2.waitKey(1)\n",
    "    if(key == ord('q')):\n",
    "        if status is 1:\n",
    "            times.append(datetime.now())\n",
    "        break\n",
    "        \n",
    "print(status_list)\n",
    "print(times)\n",
    "\n",
    "#Store into DataFrame\n",
    "for i in range(0, len(times), 2):\n",
    "    df = df.append({\n",
    "    \"Start\":times[i],\n",
    "    \"End\":times[i+1]},\n",
    "    ignore_index = True)\n",
    "\n",
    "df.to_csv(input_path+\"Times.csv\", index = False)\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Motion Detection Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bokeh is a library that help us to visulaize  data on browser\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import HoverTool, ColumnDataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Start_string\"] = df[\"Start\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"End_string\"] =  df[\"End\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "cds=ColumnDataSource(df)\n",
    "\n",
    "p=figure(x_axis_type='datetime', height=100, width=500,  title=\"Motion Grpah\")\n",
    "p.yaxis.minor_tick_line_color = None\n",
    "p.ygrid[0].ticker.desired_num_ticks=1\n",
    "\n",
    "hover=HoverTool(tooltips=[(\"Start\",\"@Start_string\"), (\"End\", \"@End_string\")])\n",
    "p.add_tools(hover)\n",
    "\n",
    "q=p.quad(left=\"Start\", right=\"End\", bottom=0, top=1, color=\"red\", source=cds)\n",
    "\n",
    "\n",
    "output_file(input_path+\"Graph.html\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
